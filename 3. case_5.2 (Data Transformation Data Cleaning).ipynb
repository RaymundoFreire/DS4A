{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we prepare Yelp data to answer business questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from shapely.geometry import Point, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal (3 min)\n",
    "\n",
    "In this case, we will introduce you to another uncleaned dataset for practice. We will focus on the nuances of cleaning the dataset parameter by parameter. Being able to dive into a single parameter and investigate its attributes will be crucial for modeling in later stages of the data science proceess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Context.** [Yelp](https://www.yelp.com) is a very popular website, where anyone can write a review about restaurants, hotels, spas or any business. They have decided to start analysing the data and will use the results of the analysis to make decisions about new features to the service. They have now approached you, an independant data consultant with a requirement of cleaning the data they have and for you to help them get more context on the data.\n",
    "\n",
    "**Business Problem.** Your task is to go through the data, and make transformations or additions to the data based on their needs, which are listed below.\n",
    "\n",
    "**Analytical Context.** The client has shared three files with you containing details about the businesses and end-users on the service along with the reviews posted by these users. With this data, they would like you to do the following:\n",
    "\n",
    "1. For each business, find out the county and state that it is a part of\n",
    "2. For each business, calculate the total number of reviews received and the average star rating across those reviews\n",
    "3. For each business, calculate the total number of check-ins by hour of day, by day of week, and across all time\n",
    "    \n",
    "We will initially run these operations locally with a small subset of the data. We will then run the same set of operations in an EC2 instance for the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data files (3 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in all three data files provided and familiarize ourselves with the data. Note that we truncate our read to the first 10000 rows due to the sizes of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.csv', nrows=1000)\n",
    "businesses = pd.read_csv('businesses.csv', nrows=1000)\n",
    "checkins = []\n",
    "with open('checkins.json', encoding='utf-8')  as f:\n",
    "    for row in f.readlines()[:1000]:\n",
    "        checkins.append(json.loads(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns present in each of the files are:\n",
    "\n",
    "1. **reviews**\n",
    "    * **review_id** - Unique identifier for the review\n",
    "    * **business_id** - Unique identifier of the business that is being reviewed\n",
    "    * **user_id** - Unique identifier of the user who posted the review\n",
    "    * **date** - The date and time of the review\n",
    "    * **star** - Star rating for the review\n",
    "    * **text** - Review text\n",
    "    * **cool** - Number of cool votes received for the review\n",
    "    * **funny** - Number of funny votes received for the review\n",
    "    * **useful** - Number of funny votes received for the review\n",
    "    \n",
    "    \n",
    "2. **businesses**\n",
    "    * **business_id** - Unique identifier of the business\n",
    "    * **name** - Name of the business\n",
    "    * **categories** - Categories associated with the business\n",
    "    * **latitude** - Location of the business (latitude)\n",
    "    * **longitude** - Location of the business (longitude)\n",
    "    * **review_count** - Number of reviews received for the business\n",
    "    * **stars** - Average star rating, rounded to half stars\n",
    "    \n",
    "    \n",
    "3. **checkins**\n",
    "    * **business_id** - Unique identifier of the business\n",
    "    * **date** - List of datetimes when users checked in to the business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling null values in `reviews` (3 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, handling null values is a staple of cleaning datasets. Let's uncover the columns in the table `reviews` that contain null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "cool            True\n",
       "date            True\n",
       "funny           True\n",
       "review_id      False\n",
       "stars           True\n",
       "text           False\n",
       "useful          True\n",
       "user_id        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that null values are present in the following columns: `date`, `stars`, `cool`, `funny`, and `useful`. Each of these columns need to be handled in a different way.\n",
    "\n",
    "In contrast to the previous case, here we will look at some advanced methods for handling null values that don't use normal `pandas` operations. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `date` (5 min)\n",
    "\n",
    "Since the client has indicated they will likely be using the cleaned data for numerous analyses later on, removing rows with missing values or even writing in meaningless filler values will not work. So we need to figure out a sensible interpolation method.\n",
    "\n",
    "### Exercise 1: (5 min)\n",
    "\n",
    "Given what we know about the data so far, describe a sensible interpolation method that we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** From exploring the available data, we can surmise that reviews are ordered sequentially based on the `date` field. (Can you come up with a way to verify this via code?) Thus, we can fill in a reasonable value by interpolating between the immediate previous and immediate next records that are non-null.\n",
    "\n",
    "However, our approach needs to handle cases where there are consecutive null values (we cannot always assume that the following row and the previous row are both not missing - sometimes there are long stretches of missing values!) In those cases, we will need to get the next and previous available dates which are non-null and perform the interpolation. A naive but reasonable assumption is that consecutive missing values are uniformly distributed within that interval, so we can do a simple linear interpolation based on the current row's relative position within its block of missing values. For example, if the current row is the 3rd row with a missing value in a block of 10 rows with missing values, then it should get an interpolated date which is $ \\displaystyle\\frac {3}{10 + 1} = \\frac {3} {11} $ - ths of the way from the previous non-null date to the next non-null date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code that implements our idea above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = reviews[reviews['date'].isnull()]\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "for index, row in nan_rows.iterrows():\n",
    "    previous_date = reviews.iloc[index - 1]['date']\n",
    "    if isinstance(previous_date, str):\n",
    "        previous_date = datetime.datetime.strptime(previous_date, date_format)\n",
    "    next_date = None\n",
    "    next_date_count = 1\n",
    "    while next_date is None:\n",
    "        try:\n",
    "            next_date = datetime.datetime.strptime(reviews.iloc[index + next_date_count]['date'], date_format)\n",
    "        except Exception:\n",
    "            next_date_count += 1\n",
    "    difference = (next_date - previous_date).seconds\n",
    "    current_date = previous_date + datetime.timedelta(seconds=difference/(next_date_count + 1))\n",
    "    reviews.loc[index, 'date'] = current_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `star` (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next feature which has null values to handle is `star`. `star` is a numeric value and can range from 1 to 5, where 1 represents the lowest rating and 5 the highest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: (10 min)\n",
    "\n",
    "Describe an appropriate method for filling in the missing values of `star`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** Since we do not know how the client will want to use the values of `star` in later analysis, we do not want to use a naive method of filling in the values such as replacing nulls with the mean or median of the non-missing values. Doing so could compromise any analysis of the distribution of star ratings, as well as investigations of the relationships between star ratings and other variables.\n",
    "\n",
    "One method that we discussed in the previous case, which is often a good choice, is to impute the missing values as a function of the other (non-missing) features of that row. For example, we could impute `star` given the values of `cool`, `date`, `funny`, `useful`, `text`. But this is not necessarily the wisest choice. For one thing, if those 5 variables are poor predictors of `star`, and in fact `star` is most closely related to other variables that we have not yet been provided, then we are pegging all future analyses involving `star` to a subpar interpolation moodel. And even if these variables are very good predictors of `star`, perhaps the client is most interested in examining outliers, which by definition won't be the values being filled in by the model.\n",
    "\n",
    "Therefore, the most prudent option which will not alter subsequent summary statistics calculations and not skew the distribution of the non-missing data would be to replace all missing values with a standard `NaN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `cool`, `funny`, `useful` and revisiting interpolation (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a similar logic to conclude that the `cool`, `funny`, and `useful` columns should also be filled with `NaN`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['cool'].fillna(np.nan, inplace=True)\n",
    "reviews['funny'].fillna(np.nan, inplace=True)\n",
    "reviews['useful'].fillna(np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume, for the sake of progress, that the inputs `cool`, `date`, `funny`, `useful`, and `text` ARE good predictors for `star`, and that the client is mostly interested in high-level properties of the distributions of these variables and how they correlate to each other. In such a case, we can go ahead and leverage the power of machine learning to predict a reasonable estimate for `star`. \n",
    "\n",
    "We will do so by using the [IterativeImputer](https://scikit-learn.org/stable/modules/impute.html#multivariate-feature-imputation) class of `scikit-learn` to fill the missing values in these columns. `IterativeImputer` fills one column after another by building models for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "cool            True\n",
       "date           False\n",
       "funny           True\n",
       "review_id      False\n",
       "stars           True\n",
       "text           False\n",
       "useful          True\n",
       "user_id        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "cool            True\n",
       "date           False\n",
       "funny           True\n",
       "review_id      False\n",
       "stars          False\n",
       "text           False\n",
       "useful          True\n",
       "user_id        False\n",
       "index          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['index'] = reviews.index\n",
    "imputer = IterativeImputer()\n",
    "imputed_df = pd.DataFrame(imputer.fit_transform(reviews[['index', 'stars']]))\n",
    "imputed_df\n",
    "imputed_df.columns = ['index', 'stars']\n",
    "imputed_df.index = reviews.index\n",
    "reviews[['stars']] = imputed_df['stars']\n",
    "reviews.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value has to be dynamically filled, similar to the cool and funny columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with erroneous values\n",
    "\n",
    "Of course, missing values are not the only problem in datasets – erroneous values are too! So we should look a the other columns with non-null values to see if they could have errors that we can correct. Here, the other columns are `business_id`, `review_id`, `user_id`, and `text`. However, IDs are arbitrary identifiers which we have no way of ascertaining if they are correct or not, so we have to take the values we are given for granted.\n",
    "\n",
    "`text` is the text of the user review, but the objective of any attempts to clean this is unclear - what metrics would we use to determine if a review text is \"clean\" or not? The answer to this question is not obvious at all without much more precise direction from the client about what they intend to use this text for, so we will leave it alone for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up `businesses` (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking which columns contain null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "categories      True\n",
       "city           False\n",
       "latitude       False\n",
       "longitude      False\n",
       "name           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are null values in `categories`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: (5 min)\n",
    "\n",
    "Suppose for a moment that there were missing values in the `city` column. Can you describe a method which would allow us to effectively fill in missing `city` values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** Since we have the exact location coordinates (latitude and longitude) for each business, we can use an external dataset which contains the coordinates of the center of each city in the United States. We can then compute the distances between a particular business's coordinates and each city's coordinates and find the closest one, and fill in the missing value with that closest city's name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: (5 min)\n",
    "\n",
    "Describe and implement the best method to fill in the null values in the `categories` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** Since there are no other indicators in this file that could help determine the category of a restaurant, the null values in this column should be filled with a default replacement value like \"Not Found\", as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses['categories'].fillna('Not found', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that this is not an ideal solution as those businesses do have a category associated with them; we just do not know what it is. It could be random that some businesses were not documented with a category but it is also possible that we are dealing with a systematic error here. For example, perhaps all the businesses in a specific area were not documented. Our default solution is the best option we have, but a good data scientist should note when his/her imputation methods are subject to the possibility of high errors and what additional information they would need to address those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another look at erroneous values\n",
    "\n",
    "Of course, missing values are not the only thing that can go wrong in a dataset – erroneous values can affect the analysis as well. Looking at the columns which we have not dealt with yet, we have `business_id`, `latitude`, `longitude`, and `name` as potential candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: (5 min)\n",
    "\n",
    "Describe and implement suitable methods to deal with potential erroneous values in these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** `business_id` is just an arbitrarily assigned identifier, so we have no way of knowing if a non-missing value is wrong; we just have to take it for granted. The same goes with the `name` column. However, there are some basic rules we can apply to `latitude` and `longitude` to determine if they are off.\n",
    "\n",
    "By basic interpretation of what latitude and longitude means, we know that the latitude values should be in the range `[-90, 90]` and longitude should be in the range `[-180, 180]`. So we should check that all data points have latitude and longitude values that lie within these ranges. When they are outside the range, we should replace them with acceptable nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.loc[(businesses['latitude'] < -90) | (businesses['latitude'] > 90), 'latitude'] = np.nan\n",
    "businesses.loc[(businesses['longitude'] < -180) | (businesses['longitude'] > 180), 'longitude'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding information for `businesses` (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the client's request, we must find the county and state each business belongs in and add this to the table. We use the geoJSON files that contain the geoshapes of each state and county in the US. We will be using the [shapely](https://shapely.readthedocs.io/en/latest/manual.html) library to figure out whether a point is present inside the shape that represents the border of each state/county:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('us-state-shapes.json') as f:\n",
    "    states = json.load(f)\n",
    "    \n",
    "def get_state_name(row):\n",
    "    if not row['latitude'] or not row['longitude']:\n",
    "        return None\n",
    "    point = Point(row['longitude'], row['latitude'])\n",
    "    for state in states['features']:\n",
    "        polygon = shape(state['geometry'])\n",
    "        if polygon.contains(point):\n",
    "            return state['properties']['NAME']\n",
    "        \n",
    "businesses['state'] = businesses.apply(get_state_name, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: (5 min)\n",
    "\n",
    "Find the county of each business and add them to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('us-county-shapes.json') as f:\n",
    "    counties = json.load(f)\n",
    "    \n",
    "def get_county_name(row):\n",
    "    if not row['latitude'] or not row['longitude']:\n",
    "        return None\n",
    "    point = Point(row['longitude'], row['latitude'])\n",
    "    for county in counties['features']:\n",
    "        polygon = shape(county['geometry'])\n",
    "        if polygon.contains(point):\n",
    "            return county['properties']['NAME']\n",
    "        \n",
    "businesses['county'] = businesses.apply(get_county_name, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add information about a restaurant's number of reviews and their average star rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: (5 min)\n",
    "\n",
    "Calculate the number of reviews received for each restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_review_count(row):\n",
    "    business_id = row['business_id']\n",
    "    business_reviews = reviews[reviews['business_id'] == business_id]\n",
    "    return len(business_reviews)\n",
    "\n",
    "businesses['review_count'] = businesses.apply(calculate_review_count, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: (5 min)\n",
    "\n",
    "For each restaurant, calculate its avearge star rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_stars(row):\n",
    "    business_id = row['business_id']\n",
    "    business_reviews = reviews[reviews['business_id'] == business_id]\n",
    "    average_stars = business_reviews['stars'].mean()\n",
    "    return average_stars\n",
    "\n",
    "businesses['stars'] = businesses.apply(calculate_average_stars, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating the check-ins data (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do our final task: use the check-ins data given in the JSON file to compute various statistics on the number oof check-ins over various time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: (20 min)\n",
    "\n",
    "Compute, for each business, the total number of check-ins by hour of day, by weekday, by day, and across all time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_aggregations = []\n",
    "for checkin in checkins:\n",
    "    business_info = {\n",
    "        'id': checkin['business_id'],\n",
    "        'total_checkins': len(checkin['date']),\n",
    "        'checkins_by_day': {},\n",
    "        'checkins_by_hour': {},\n",
    "        'checkins_by_weekday': {},\n",
    "    }\n",
    "    for date in checkin['date'].split(','):\n",
    "        date = datetime.datetime.strptime(date.strip(), date_format)\n",
    "        day_str = date.strftime('%Y-%m-%d')\n",
    "        checkin_for_day = business_info['checkins_by_day'].get(day_str, 0)\n",
    "        checkin_for_day += 1\n",
    "        business_info['checkins_by_day'][day_str] = checkin_for_day\n",
    "        hour_checkins = business_info['checkins_by_hour'].get(date.hour, 0)\n",
    "        hour_checkins += 1\n",
    "        business_info['checkins_by_hour'][date.hour] = hour_checkins\n",
    "        weekday_checkins = business_info['checkins_by_weekday'].get(date.weekday(), 0)\n",
    "        weekday_checkins += 1\n",
    "        business_info['checkins_by_weekday'][date.weekday()] = weekday_checkins\n",
    "    checkin_aggregations.append(business_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy notebook and data files to EC2 instance - Take Home Task (5 min)\n",
    "\n",
    "We have performed these operations only on the first 10000 rows of our data tables. We will set up a Jupyter Notebook in an EC2 instance to apply these operations to the entire dataset. We need to open port 8889 on the EC2 instance.\n",
    "\n",
    "Now, the process will not take as long this time, because we have already done most of the setup in the previous case. Let's copy the notebook file and the data files from the local machine to the EC2 instance. It can be done via an scp command, or using winSCP. There are different ways to SCP (secure copy) the file into the server:\n",
    "\n",
    "1. If you are using a Linux or a Mac, type the following command to copy the file:\n",
    "\n",
    "```\n",
    "scp -i <path_to_pem_file> </path/to/jupyter/notebook> <username>@<EC2IP>:</destination/path>\n",
    "```\n",
    "\n",
    "2. If you are using Windows, you can either use WinSCP - https://winscp.net/eng/index.php or if you are using Putty, you can use the pscp command:\n",
    "\n",
    "```\n",
    "pscp -i <path_to_ppk_file> <\\path\\to\\jupyter\\notebook\\> <username>@<EC2IP>:</destination/path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the entire set with full data in the server - Take Home Task (5 min)\n",
    "\n",
    "Once the file is copied, you can now access the jupyter notebook from `http:server_ip:8889`. Once you are able to access the file, just remove the `nrows=10000` parameter from the [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function call, in order to read the entire dataset. You can now click on `Cells > Run All` in order to run the same set of steps on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions (2 min)\n",
    "\n",
    "In this case, we practiced our skills in dealing with missing values and also learned about common ways of handling erroneous values. We then creatively leveraged external data in order to engineer additional features based on client requests. We again took a small local sample of the overall data, then uploaded our work to Amazon EC2 so that they could be applied to the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways (8 min)\n",
    "\n",
    "In this case, you looked more at how to deal with missing data. You learned that although interpolation is a powerful tool, when and how to use it depends highly on the projected use cases of the data. Sometimes, it is better to simply replace a missing value with \"Not found\" or `NaN` so that it is clear to end users to discount it appropriately from subsequent analyses, rather than run the risk of skewing the resultant data distribution or summary statistics.\n",
    "\n",
    "You also learned a few ways of using existing public data to help interpolate missing values in your current dataset. This is an invaluable skill and often times the best way to deal with missing values is to exercise this resourcefulness. It is important to recognize not all interpolation is made equal. If you have many parameters that are imputed poorly or with great uncertainty, it will be much harder to develop an accurate prediction model later if those parameters are key predictors.\n",
    "\n",
    "Finally, you looked at erroneous values and various common ways these could leak into the data. You also learned that sometimes it is too difficult or impossible to systematically determine existence of error.\n",
    "    \n",
    "We dove into the technical nuances of data cleaning in this case but it is important to note this dataset is very simple compared to the data you will be working with in real life. It is not too uncommon to deal with datasets with hundreds of parameters where dozens have different types of missing/incorrect data. Additionally, you may encounter datasets where you are unsure what some columns/parameters mean. This dataset came with a nice handy dictionary explaining what information is in each column. \n",
    "    \n",
    "Students are highly encouraged to thoroughly review all the EDA-related cases we have taught up to this point. Mastery of EDA will immensely improve the quality of your data wrangling & cleaning process, which in combination will impact your subsequent modeling process in a positive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
